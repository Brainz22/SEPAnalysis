{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"Datasets/OriginalData.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34039"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to put USAF datasets into one full dataframe, which is achieved by the following loop. \"USAFDfs\" will be a list of dataframes from each year spanning 1981-2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAFDfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=0\n",
    "n = 8\n",
    "while(len(USAFDfs) <= 14 ):\n",
    "  q += 1\n",
    "  if (q >= 10):\n",
    "    q -= 10\n",
    "    n = 9\n",
    "  Df = pd.read_csv(\"Datasets/USAF/DfEd3_19{}{}_usaf.csv\".format(n,q))\n",
    "  USAFDfs.append(Df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(USAFDfs)):\n",
    "  USAFDfs[i].pop(\"Unnamed: 0\");\n",
    "  USAFDfs[i].pop('OBS');\n",
    "  USAFDfs[i].pop('Timetag');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating dataframes on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(USAFDfs)):\n",
    "  df = pd.concat([df, USAFDfs[i]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries is: 66361\n"
     ]
    }
   ],
   "source": [
    "# Data frame with columns sorted by ARNUMBER so that we can easily visualize the changes of each AR\n",
    "DF = df.sort_values(by=['ARNUMBER', 'LOND'], ignore_index=True)\n",
    "SmallDf = DF#.head(30) #Using 10 elements of original data\n",
    "print(\"Number of entries is:\",len(DF))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every column of small Df is turned into a series, then into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Separating the columns into list \n",
    "    \n",
    "Obs_ID = SmallDf['OBS_ID'].squeeze()\n",
    "Obs_ID_List = Obs_ID.tolist()\n",
    "Timetag = SmallDf['TIMETAG'].squeeze()\n",
    "Timetag_List = Timetag.tolist()\n",
    "ARNumber = SmallDf['ARNUMBER'].squeeze()\n",
    "ARNumber_List = ARNumber.tolist()\n",
    "Loc = SmallDf['LOC'].squeeze()\n",
    "Loc_List = Loc.tolist()\n",
    "Latd = SmallDf['LATD'].squeeze()\n",
    "Latd_List = Latd.tolist()\n",
    "Lond = SmallDf['LOND'].squeeze()\n",
    "Lond_List = Lond.tolist()\n",
    "Lonh = SmallDf['LONH'].squeeze()\n",
    "Lonh_List = Lonh.tolist()\n",
    "Area = SmallDf['AREA'].squeeze()\n",
    "Area_List = Area.tolist()\n",
    "Z = SmallDf['Z'].squeeze()\n",
    "Z_List = Z.tolist()\n",
    "LL = SmallDf['LL'].squeeze()\n",
    "LL_List = LL.tolist()\n",
    "NN = SmallDf['NN'].squeeze()\n",
    "NN_List = NN.tolist()\n",
    "MagType = SmallDf['MAGTYPE'].squeeze()\n",
    "MagType_List = MagType.tolist()\n",
    "Type = [0] #empty list to store type \"Real (0) or Prediciton (1)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, We extrapolate locations of Active Regions using Carrington Rate and we extend the active region ONLY if this is the last record for this active region in the entire series\n",
    "(i.e. only if it disappears the next day and does not appear anymore). The list \"Type\" stores 0 for non-extraploated and 1 for extrapolated entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j = 0\n",
    "while(True):\n",
    "    if(j>=len(Lond_List)): break\n",
    "    #if((Lond_List[j]+14.2) <= 270.0): #If current pos. does not surpass 270 when +14.2, then insert a new\n",
    "    #   #a new with +14.2 \n",
    "    # ---------------------------------------------------------------\n",
    "    # Let's extend the active region ONLY if this is the last record for this active region in the entire series\n",
    "    # (i.e. only if it disappears the next day and does not appear anymore).\n",
    "    # This leads to a slightly modified cycle condition:\n",
    "    # if (longitude+14.2 < 270) AND (no ARs with similar numbers presented further)\n",
    "    # ---------------------------------------------------------------\n",
    "    if ( ((Lond_List[j]+14.2) <= 270.0) and (ARNumber_List[j] not in ARNumber_List[j+1:]) ):\n",
    "        Lond_List.insert(j+1, (Lond_List[j]+14.2))\n",
    "        ARNumber_List.insert(j+1, ARNumber_List[j])\n",
    "        Obs_ID_List.insert(j+1, Obs_ID_List[j])\n",
    "        Latd_List.insert(j+1, Latd_List[j])\n",
    "        Lonh_List.insert(j+1, Lonh_List[j])\n",
    "        Area_List.insert(j+1, Area_List[j])\n",
    "        Z_List.insert(j+1, Z_List[j])\n",
    "        LL_List.insert(j+1, LL_List[j])\n",
    "        NN_List.insert(j+1, NN_List[j])\n",
    "        MagType_List.insert(j+1, MagType_List[j])\n",
    "        Timetag_List.insert(j+1, Timetag_List[j]) #Then, convert to DF timestamp and iterate to apply DateOffSet\n",
    "        Loc_List.insert(j+1, Loc_List[j])\n",
    "        \n",
    "        #Missing Timestags and Location\n",
    "        \n",
    "        Type.insert(j+1, 1)\n",
    "        j+=1\n",
    "        \n",
    "    else:\n",
    "        if((j+1) < len(Lond_List)): #So that it will not go past the the number of total elements\n",
    "            Type.insert(j+1, 0) #Type list is empty, so I need to add 0's too.\n",
    "            #MagType_List\n",
    "            \n",
    "            \n",
    "            #ARNumber_List.insert(j+1, ARNumber_List[j+1]) --> Don't need because there already different\n",
    "               ##.  AR numbers for the real data. So, a real data is identified just iterate to next element\n",
    "                ##   and begin repeating AR numbers then.\n",
    "        j+=1\n",
    "        \n",
    "        \n",
    "#Lond_List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loop did not update the dates but only the locating if the 14.2 degrees were added. Thus, we need to add an extra day for every 14.2 degrees as required by the Carrington rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1982-04-24 00:00:00')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFdate = pd.DataFrame({'Timetag': Timetag_List, 'Type': Type})\n",
    "DFdate['Timetag'] = pd.to_datetime(DFdate['Timetag'])\n",
    "\n",
    "for index, row in DFdate.iterrows():\n",
    "    if (row['Type'] == 0):\n",
    "        dt = 0 \n",
    "        continue\n",
    "    else:\n",
    "        dt+=1\n",
    "        if dt == 1:\n",
    "            txt = \"{} day\".format(dt)\n",
    "            \n",
    "        else: \n",
    "            txt = \"{} days\".format(dt)\n",
    "            \n",
    "            \n",
    "    DFdate.iloc[index, 0] = row['Timetag'] + pd.Timedelta(txt)#+ pd.Timedelta(txt)\n",
    "\n",
    "DFdate.head(50) #50 entries of the new updated dates\n",
    "Updated_Timetag_Series = DFdate['Timetag']\n",
    "#Updated_Timetag_Series.dt.date\n",
    "Updated_Timetag_List = Updated_Timetag_Series.tolist()\n",
    "Updated_Timetag_List[10] #List of updated dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The McIntosh classes that appear in upper case are turned to lower cases for the second and third components and putting. Then, they are stored in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = ['A', 'B', 'C', 'D', 'E', 'F', 'H']\n",
    "LowPen = ['a', 'h', 'k', 'r', 's', 'x']  #Lower case Penumbra\n",
    "CapPen = ['A','H','K','R','S','X']\n",
    "LowDist = ['i', 'c', 'o', 'x']\n",
    "CapDist = ['I','C','O','X']\n",
    "z = []\n",
    "for i in range (len(Z_List)):\n",
    "    z.append(list(Z_List[i])) #Z_List contains strings. list() converts to list'z' is a tuple. I have list with len(Z_List) and 3 columns\n",
    "for i in range(len(z)):\n",
    "    for j in range(len(LowPen)):\n",
    "        if z[i][1] == CapPen[j]:\n",
    "            z[i][1] = LowPen[j]\n",
    "            break\n",
    "            \n",
    "for i in range(len(z)):\n",
    "    for j in range(len(LowDist)):\n",
    "        if z[i][2] == CapDist[j]:\n",
    "            z[i][2] = LowDist[j]\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code overwrites the locations. Note that this list will not be important when performing our statistical study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwrites Loc_List\n",
    "\n",
    "# Adds the location North, South, East, and West accordingly.\n",
    "\n",
    "# ---------> ISSUE: At this point, I am adding 90+ degrees as West. [90, 180] should be West too, but\n",
    "##.      [180, 270] should East, if I am correct. I tried using 'or' inside the if statement to fix it,\n",
    "##.      but it seems like it does not do its job. I can certainly fix it, if I keep playing with it.\n",
    "\n",
    "\n",
    "Lond_List = [round(x) for x in Lond_List] #convert to integer\n",
    "for i in range(len(Lond_List)):\n",
    "    if (Latd_List[i] < 0 and Lond_List[i] < 0):\n",
    "        loc = \"S{}E{}\".format(Latd_List[i]*-1, Lond_List[i]*-1)\n",
    "    elif (Latd_List[i] < 0 and Lond_List[i] > 0):\n",
    "        loc = \"S{}W{}\".format(Latd_List[i]*-1, Lond_List[i])\n",
    "    elif (Latd_List[i] > 0 and (Lond_List[i] < 0 or Lond_List[i] > 180) ):\n",
    "        loc = \"N{}E{}\".format(Latd_List[i], -1*Lond_List[i])\n",
    "    elif (Latd_List[i] > 0 and (Lond_List[i] > 0 or Lond_List[i] < 180)):\n",
    "        loc = \"N{}W{}\".format(Latd_List[i], Lond_List[i])\n",
    "        \n",
    "    Loc_List[i] = loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = {'OBS_ID': Obs_ID_List, 'TIMETAG': Updated_Timetag_List, 'ARNUMBER': ARNumber_List, 'LOC': Loc_List, \\\n",
    "       'LATD': Latd_List, 'LOND': Lond_List, 'LONH': Lonh_List, 'AREA': Area_List, 'Z': Z_List, \\\n",
    "        'z': z,'LL': LL_List, 'NN': NN_List, 'MAGTYPE': MagType_List, 'REAL(0)/PRED(1)': Type}\n",
    "\n",
    "Database = pd.DataFrame(Dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our \"Database\", we need to rewrite McIntosh components to keep all McIntosh records consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXX' 'BXO' 'FKI' 'FHI' 'EKO' 'EHO' 'EHI' 'CHO' 'CSO' 'DAI' 'EAI' 'DSO'\n",
      " 'HHX' 'DAO' 'HSX' 'CAO' 'DHO' 'DSI' 'EKI' 'EKC' 'DKI' 'DRO' 'DKO' 'EAO'\n",
      " 'HKX' 'HAX' 'HRX' 'CKO' 'DHI' 'DKC' 'CKI' 'CRO' 'DRI' 'CHI' 'ESO' 'CSI'\n",
      " 'FKC' 'FAO' 'CAI' 'CRI' 'FHO' 'FSO' 'ESI' 'EAC' 'FSI' 'FAI' 'EHC' 'DHC'\n",
      " 'ERO' 'FKO' 'DSC' 'ERI' 'FRO' 'DAC' 'FAC' 'ESC' 'FSC' 'FHC' 'BXI' 'Hrx'\n",
      " 'Axx' 'Hsx' 'Hax' 'Eki' 'Esi' 'Eai' 'Eso' 'Eao' 'Fhi' 'Cao' 'Cso' 'Dso'\n",
      " 'Dao' 'Dai' 'Cro' 'Bxo' 'Dsi' 'Fso' 'Dko' 'Dki' 'Dro' 'Fao' 'Eko' 'Fac'\n",
      " 'Fai' 'Fki' 'Eho' 'Fkc' 'Hkx' 'Cho' 'Dho' 'Dac' 'Fsi' 'Ekc' 'Fho' 'Cko'\n",
      " 'Cai' 'Csi' 'Fko' 'Eac' 'Dkc' 'Hhx' 'Ehi' 'Dri' 'Dhi' 'Fro' 'Chi' 'Dhc'\n",
      " 'Ckc' 'Esc' 'Hxx' 'Dsc' 'Bxi' 'Fsc' 'Ero' 'Fhc' 'Ehc' 'Cki' 'Drc' 'Cri'\n",
      " 'Hso' 'Eri']\n"
     ]
    }
   ],
   "source": [
    "print(Database.Z.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database.loc[Database['Z'] == 'Hrx', 'Z'] = \"HRX\"\n",
    "Database.loc[Database['Z'] == 'Axx', 'Z'] = \"AXX\"\n",
    "Database.loc[Database['Z'] == 'Hsx', 'Z'] = \"HSX\"\n",
    "Database.loc[Database['Z'] == 'Hax', 'Z'] = \"HAX\"\n",
    "Database.loc[Database['Z'] == 'Eki', 'Z'] = \"EKI\"\n",
    "Database.loc[Database['Z'] == 'Esi', 'Z'] = \"ESI\"\n",
    "Database.loc[Database['Z'] == 'Eai', 'Z'] = \"EAI\"\n",
    "Database.loc[Database['Z'] == 'Eso', 'Z'] = \"ESO\"\n",
    "Database.loc[Database['Z'] == 'Eao', 'Z'] = \"EAO\"\n",
    "Database.loc[Database['Z'] == 'Fhi', 'Z'] = \"FHI\"\n",
    "Database.loc[Database['Z'] == 'Cao', 'Z'] = \"CAO\"\n",
    "Database.loc[Database['Z'] == 'Cso', 'Z'] = \"CSO\"\n",
    "Database.loc[Database['Z'] == 'Dso', 'Z'] = \"DSO\"\n",
    "Database.loc[Database['Z'] == 'Dao', 'Z'] = \"DAO\"\n",
    "Database.loc[Database['Z'] == 'Dai', 'Z'] = \"DAI\"\n",
    "Database.loc[Database['Z'] == 'Cro', 'Z'] = \"CRO\"\n",
    "Database.loc[Database['Z'] == 'Bxo', 'Z'] = \"BXO\"\n",
    "Database.loc[Database['Z'] == 'Dsi', 'Z'] = \"DSI\"\n",
    "Database.loc[Database['Z'] == 'Fso', 'Z'] = \"FSO\"\n",
    "Database.loc[Database['Z'] == 'Dko', 'Z'] = \"DKO\"\n",
    "Database.loc[Database['Z'] == 'Dki', 'Z'] = \"DKI\"\n",
    "Database.loc[Database['Z'] == 'Dro', 'Z'] = \"DRO\"\n",
    "Database.loc[Database['Z'] == 'Fao', 'Z'] = \"FAO\"\n",
    "Database.loc[Database['Z'] == 'Eko', 'Z'] = \"EKO\"\n",
    "Database.loc[Database['Z'] == 'Fac', 'Z'] = \"FAC\"\n",
    "Database.loc[Database['Z'] == 'Fai', 'Z'] = \"FAI\"\n",
    "Database.loc[Database['Z'] == 'Fki', 'Z'] = \"FKI\"\n",
    "Database.loc[Database['Z'] == 'Eho', 'Z'] = \"EHO\"\n",
    "Database.loc[Database['Z'] == 'Fkc', 'Z'] = \"FKC\"\n",
    "Database.loc[Database['Z'] == 'Hkx', 'Z'] = \"HKX\"\n",
    "Database.loc[Database['Z'] == 'Cho', 'Z'] = \"CHO\"\n",
    "Database.loc[Database['Z'] == 'Dho', 'Z'] = \"DHO\"\n",
    "Database.loc[Database['Z'] == 'Dac', 'Z'] = \"DAC\"\n",
    "Database.loc[Database['Z'] == 'Fsi', 'Z'] = \"FSI\"\n",
    "Database.loc[Database['Z'] == 'Ekc', 'Z'] = \"EKC\"\n",
    "Database.loc[Database['Z'] == 'Fho', 'Z'] = \"FHO\"\n",
    "Database.loc[Database['Z'] == 'Cko', 'Z'] = \"CKO\"\n",
    "Database.loc[Database['Z'] == 'Cai', 'Z'] = \"CAI\"\n",
    "Database.loc[Database['Z'] == 'Csi', 'Z'] = \"CSI\"\n",
    "Database.loc[Database['Z'] == 'Fko', 'Z'] = \"FKO\"\n",
    "Database.loc[Database['Z'] == 'Eac', 'Z'] = \"EAC\"\n",
    "Database.loc[Database['Z'] == 'Dkc', 'Z'] = \"DKC\"\n",
    "Database.loc[Database['Z'] == 'Hhx', 'Z'] = \"HHX\"\n",
    "Database.loc[Database['Z'] == 'Ehi', 'Z'] = \"EHI\"\n",
    "Database.loc[Database['Z'] == 'Dri', 'Z'] = \"DRI\"\n",
    "Database.loc[Database['Z'] == 'Dhi', 'Z'] = \"DHI\"\n",
    "Database.loc[Database['Z'] == 'Fro', 'Z'] = \"FRO\"\n",
    "Database.loc[Database['Z'] == 'Chi', 'Z'] = \"CHI\"\n",
    "Database.loc[Database['Z'] == 'Dhc', 'Z'] = \"DHC\"\n",
    "Database.loc[Database['Z'] == 'Ckc', 'Z'] = \"CKC\"\n",
    "Database.loc[Database['Z'] == 'Esc', 'Z'] = \"ESC\"\n",
    "Database.loc[Database['Z'] == 'Hxx', 'Z'] = \"HXX\"\n",
    "Database.loc[Database['Z'] == 'Dsc', 'Z'] = \"DSC\"\n",
    "Database.loc[Database['Z'] == 'Bxi', 'Z'] = \"BXI\"\n",
    "Database.loc[Database['Z'] == 'Fsc', 'Z'] = \"FSC\"\n",
    "Database.loc[Database['Z'] == 'Ero', 'Z'] = \"ERO\"\n",
    "Database.loc[Database['Z'] == 'Fhc', 'Z'] = \"FHC\"\n",
    "Database.loc[Database['Z'] == 'Ehc', 'Z'] = \"EHC\"\n",
    "Database.loc[Database['Z'] == 'Cki', 'Z'] = \"CKI\"\n",
    "Database.loc[Database['Z'] == 'Drc', 'Z'] = \"DRC\"\n",
    "Database.loc[Database['Z'] == 'Cri', 'Z'] = \"CRI\"\n",
    "Database.loc[Database['Z'] == 'Hso', 'Z'] = \"HSO\"\n",
    "Database.loc[Database['Z'] == 'Eri', 'Z'] = \"ERI\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note below how they are all now consistent on column 'Z,' i.e. all were turned into capital case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXX' 'BXO' 'FKI' 'FHI' 'EKO' 'EHO' 'EHI' 'CHO' 'CSO' 'DAI' 'EAI' 'DSO'\n",
      " 'HHX' 'DAO' 'HSX' 'CAO' 'DHO' 'DSI' 'EKI' 'EKC' 'DKI' 'DRO' 'DKO' 'EAO'\n",
      " 'HKX' 'HAX' 'HRX' 'CKO' 'DHI' 'DKC' 'CKI' 'CRO' 'DRI' 'CHI' 'ESO' 'CSI'\n",
      " 'FKC' 'FAO' 'CAI' 'CRI' 'FHO' 'FSO' 'ESI' 'EAC' 'FSI' 'FAI' 'EHC' 'DHC'\n",
      " 'ERO' 'FKO' 'DSC' 'ERI' 'FRO' 'DAC' 'FAC' 'ESC' 'FSC' 'FHC' 'BXI' 'CKC'\n",
      " 'HXX' 'DRC' 'HSO']\n"
     ]
    }
   ],
   "source": [
    "print(Database.Z.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to normalize all values for the purpose of machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxArea = Database.AREA.max()\n",
    "NORMAREA = Database['AREA'] / MaxArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxLond = Database.LOND.max()\n",
    "NORMLOND = Database['LOND'] / MaxLond"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting Normalized Area and Longitude into database (ONLY needed to be run once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database.insert(8, \"NORMAREA\", NORMAREA)\n",
    "Database.insert(6, \"NORMLOND\", NORMLOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttag = Database.TIMETAG.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database['TIMETAG'] = Database['TIMETAG'].dt.strftime('%Y-''%m-''%d ' '%r') #Date and time to string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below produces the json file used in our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database.to_json(\"Datasets/ARDatabase.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
